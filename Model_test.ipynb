{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stock data from Quandl API\n",
    "##change this to Alpaca if we do minute-by-minute\n",
    "import quandl\n",
    "quandl.ApiConfig.api_key = \"tC5AjCkyqtgbMarYtnVU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set dates between which stock data will be pulled for a given ticker\n",
    "START_PULL = \"2005-01-01\"\n",
    "END_PULL = \"2018-02-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Adobe\n",
    "adobeSymbol = \"WIKI/ADBE\"\n",
    "adobeDf = quandl.get(adobeSymbol, start_date= START_PULL, end_date= END_PULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2798\n",
      "3292\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj. Open</th>\n",
       "      <th>Adj. High</th>\n",
       "      <th>Adj. Low</th>\n",
       "      <th>Adj. Close</th>\n",
       "      <th>Adj. Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-03</th>\n",
       "      <td>31.494084</td>\n",
       "      <td>31.744037</td>\n",
       "      <td>30.769220</td>\n",
       "      <td>30.839207</td>\n",
       "      <td>5508800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>31.099158</td>\n",
       "      <td>31.184142</td>\n",
       "      <td>29.674425</td>\n",
       "      <td>30.024360</td>\n",
       "      <td>7515400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-05</th>\n",
       "      <td>30.044356</td>\n",
       "      <td>30.444281</td>\n",
       "      <td>29.859391</td>\n",
       "      <td>29.859391</td>\n",
       "      <td>3566600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-06</th>\n",
       "      <td>30.109344</td>\n",
       "      <td>30.164333</td>\n",
       "      <td>29.239507</td>\n",
       "      <td>29.364484</td>\n",
       "      <td>6159600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-07</th>\n",
       "      <td>29.634433</td>\n",
       "      <td>29.749411</td>\n",
       "      <td>28.844581</td>\n",
       "      <td>29.384480</td>\n",
       "      <td>8512400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-26</th>\n",
       "      <td>198.500000</td>\n",
       "      <td>201.550000</td>\n",
       "      <td>198.250000</td>\n",
       "      <td>201.300000</td>\n",
       "      <td>2262478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-29</th>\n",
       "      <td>200.110000</td>\n",
       "      <td>200.860000</td>\n",
       "      <td>197.870000</td>\n",
       "      <td>198.230000</td>\n",
       "      <td>1876216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-30</th>\n",
       "      <td>197.250000</td>\n",
       "      <td>197.730000</td>\n",
       "      <td>194.890000</td>\n",
       "      <td>196.900000</td>\n",
       "      <td>3034232.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>197.130000</td>\n",
       "      <td>200.960000</td>\n",
       "      <td>196.750000</td>\n",
       "      <td>199.760000</td>\n",
       "      <td>2514574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>199.120000</td>\n",
       "      <td>201.750000</td>\n",
       "      <td>198.084500</td>\n",
       "      <td>199.380000</td>\n",
       "      <td>2353675.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3292 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Adj. Open   Adj. High    Adj. Low  Adj. Close  Adj. Volume\n",
       "Date                                                                   \n",
       "2005-01-03   31.494084   31.744037   30.769220   30.839207    5508800.0\n",
       "2005-01-04   31.099158   31.184142   29.674425   30.024360    7515400.0\n",
       "2005-01-05   30.044356   30.444281   29.859391   29.859391    3566600.0\n",
       "2005-01-06   30.109344   30.164333   29.239507   29.364484    6159600.0\n",
       "2005-01-07   29.634433   29.749411   28.844581   29.384480    8512400.0\n",
       "...                ...         ...         ...         ...          ...\n",
       "2018-01-26  198.500000  201.550000  198.250000  201.300000    2262478.0\n",
       "2018-01-29  200.110000  200.860000  197.870000  198.230000    1876216.0\n",
       "2018-01-30  197.250000  197.730000  194.890000  196.900000    3034232.0\n",
       "2018-01-31  197.130000  200.960000  196.750000  199.760000    2514574.0\n",
       "2018-02-01  199.120000  201.750000  198.084500  199.380000    2353675.0\n",
       "\n",
       "[3292 rows x 5 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cut-off for where data transitions from training set to test set\n",
    "percent_train = 0.85\n",
    "TRAIN_SPLIT = int(percent_train*len(adobeDf))\n",
    "print(TRAIN_SPLIT)\n",
    "\n",
    "full_data = len(adobeDf)\n",
    "print(full_data)\n",
    "\n",
    "features_considered = ['Adj. Open', 'Adj. High', 'Adj. Low', 'Adj. Close', 'Adj. Volume']\n",
    "\n",
    "features = adobeDf[features_considered]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.62426727, -0.63460294, -0.64110267, -0.66260519, -0.05483864],\n",
       "       [-0.64686887, -0.66642972, -0.70422182, -0.70923479,  0.39884239],\n",
       "       [-0.70723518, -0.70848653, -0.69355786, -0.71867514, -0.49395919],\n",
       "       ...,\n",
       "       [ 8.86194321,  8.8007382 ,  8.82109939,  8.84022113, -0.61432462],\n",
       "       [ 8.85507561,  8.984345  ,  8.92833564,  9.00388459, -0.73181638],\n",
       "       [ 8.9689633 ,  9.02925193,  9.00527476,  8.9821391 , -0.76819475]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scale features before training NN by subtractic mean and dividing by stdev of each feature\n",
    "#or use tf.keras.utils.normalize to rescale values to [0,1]\n",
    "dataset = features.values\n",
    "data_mean = dataset[:TRAIN_SPLIT].mean(axis = 0)\n",
    "data_std = dataset[:TRAIN_SPLIT].std(axis = 0)\n",
    "\n",
    "dataset = (dataset-data_mean)/data_std\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                     target_size, step, single_step = False):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    start_index = start_index+history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "        \n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "        \n",
    "        if single_step:\n",
    "            labels.append(dataset[indices])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "            \n",
    "    return np.array(data), np.array(labels)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#historic data -- not entirely sure how this works since we wanna do over all of train_split\n",
    "past_history = 1\n",
    "#how far into the future we want to predict a price\n",
    "future_target = 1\n",
    "#how we group the data -- could be 5 if we group by week\n",
    "#^^in which case future target woule be 1*5\n",
    "STEP = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_single, y_train_single = multivariate_data(dataset, dataset[:,1], 0, \n",
    "                                                   TRAIN_SPLIT, past_history, future_target, STEP,\n",
    "                                                  single_step = True)\n",
    "\n",
    "x_val_single, y_val_single = multivariate_data(dataset, dataset[:, 1],\n",
    "                                               TRAIN_SPLIT, None, past_history,\n",
    "                                               future_target, STEP,\n",
    "                                               single_step=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single window of past histroy = (1, 5)\n"
     ]
    }
   ],
   "source": [
    "print('single window of past histroy = {}'.format(x_train_single[0].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "BUFFER_SIZE = len(adobeDf)\n",
    "\n",
    "train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))\n",
    "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
    "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_step_model = tf.keras.models.Sequential()\n",
    "single_step_model.add(tf.keras.layers.LSTM(32,\n",
    "                                           input_shape=x_train_single.shape[-2:]))\n",
    "single_step_model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "single_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "559/559 [==============================] - 2s 4ms/step - loss: 0.6722 - val_loss: 3.0543\n",
      "Epoch 2/10\n",
      "559/559 [==============================] - 2s 3ms/step - loss: 0.6667 - val_loss: 2.9694\n",
      "Epoch 3/10\n",
      "559/559 [==============================] - 2s 4ms/step - loss: 0.6675 - val_loss: 3.0369\n",
      "Epoch 4/10\n",
      "559/559 [==============================] - 2s 4ms/step - loss: 0.6659 - val_loss: 3.0228\n",
      "Epoch 5/10\n",
      "559/559 [==============================] - 3s 5ms/step - loss: 0.6668 - val_loss: 2.9696\n",
      "Epoch 6/10\n",
      "559/559 [==============================] - 2s 3ms/step - loss: 0.6642 - val_loss: 2.9371\n",
      "Epoch 7/10\n",
      "559/559 [==============================] - 2s 4ms/step - loss: 0.6638 - val_loss: 3.0309\n",
      "Epoch 8/10\n",
      "559/559 [==============================] - 2s 4ms/step - loss: 0.6647 - val_loss: 2.8786\n",
      "Epoch 9/10\n",
      "559/559 [==============================] - 2s 3ms/step - loss: 0.6672 - val_loss: 2.8870\n",
      "Epoch 10/10\n",
      "559/559 [==============================] - 2s 4ms/step - loss: 0.6677 - val_loss: 2.9846\n"
     ]
    }
   ],
   "source": [
    "#how often the epoch will run -- so like \n",
    "#we can do over a fraction of the data set instead of the whole thing each time\n",
    "#arbitraray-ish\n",
    "EVALUATION_INTERVAL = int(TRAIN_SPLIT/5)\n",
    "EPOCHS = 10\n",
    "\n",
    "single_step_history = single_step_model.fit(train_data_single, epochs=EPOCHS,\n",
    "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                            validation_data=val_data_single,\n",
    "                                            validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
