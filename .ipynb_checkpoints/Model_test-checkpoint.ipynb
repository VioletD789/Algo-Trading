{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stock data from Quandl API\n",
    "##change this to Alpaca if we do minute-by-minute\n",
    "import quandl\n",
    "quandl.ApiConfig.api_key = \"tC5AjCkyqtgbMarYtnVU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set dates between which stock data will be pulled for a given ticker\n",
    "START_PULL = \"2005-01-01\"\n",
    "END_PULL = \"2018-02-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Adobe\n",
    "adobeSymbol = \"WIKI/ADBE\"\n",
    "adobeDf = quandl.get(adobeSymbol, start_date= START_PULL, end_date= END_PULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2798\n",
      "3292\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj. Open</th>\n",
       "      <th>Adj. High</th>\n",
       "      <th>Adj. Low</th>\n",
       "      <th>Adj. Close</th>\n",
       "      <th>Adj. Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-03</th>\n",
       "      <td>31.494084</td>\n",
       "      <td>31.744037</td>\n",
       "      <td>30.769220</td>\n",
       "      <td>30.839207</td>\n",
       "      <td>5508800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>31.099158</td>\n",
       "      <td>31.184142</td>\n",
       "      <td>29.674425</td>\n",
       "      <td>30.024360</td>\n",
       "      <td>7515400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-05</th>\n",
       "      <td>30.044356</td>\n",
       "      <td>30.444281</td>\n",
       "      <td>29.859391</td>\n",
       "      <td>29.859391</td>\n",
       "      <td>3566600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-06</th>\n",
       "      <td>30.109344</td>\n",
       "      <td>30.164333</td>\n",
       "      <td>29.239507</td>\n",
       "      <td>29.364484</td>\n",
       "      <td>6159600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-07</th>\n",
       "      <td>29.634433</td>\n",
       "      <td>29.749411</td>\n",
       "      <td>28.844581</td>\n",
       "      <td>29.384480</td>\n",
       "      <td>8512400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-26</th>\n",
       "      <td>198.500000</td>\n",
       "      <td>201.550000</td>\n",
       "      <td>198.250000</td>\n",
       "      <td>201.300000</td>\n",
       "      <td>2262478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-29</th>\n",
       "      <td>200.110000</td>\n",
       "      <td>200.860000</td>\n",
       "      <td>197.870000</td>\n",
       "      <td>198.230000</td>\n",
       "      <td>1876216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-30</th>\n",
       "      <td>197.250000</td>\n",
       "      <td>197.730000</td>\n",
       "      <td>194.890000</td>\n",
       "      <td>196.900000</td>\n",
       "      <td>3034232.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>197.130000</td>\n",
       "      <td>200.960000</td>\n",
       "      <td>196.750000</td>\n",
       "      <td>199.760000</td>\n",
       "      <td>2514574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>199.120000</td>\n",
       "      <td>201.750000</td>\n",
       "      <td>198.084500</td>\n",
       "      <td>199.380000</td>\n",
       "      <td>2353675.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3292 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Adj. Open   Adj. High    Adj. Low  Adj. Close  Adj. Volume\n",
       "Date                                                                   \n",
       "2005-01-03   31.494084   31.744037   30.769220   30.839207    5508800.0\n",
       "2005-01-04   31.099158   31.184142   29.674425   30.024360    7515400.0\n",
       "2005-01-05   30.044356   30.444281   29.859391   29.859391    3566600.0\n",
       "2005-01-06   30.109344   30.164333   29.239507   29.364484    6159600.0\n",
       "2005-01-07   29.634433   29.749411   28.844581   29.384480    8512400.0\n",
       "...                ...         ...         ...         ...          ...\n",
       "2018-01-26  198.500000  201.550000  198.250000  201.300000    2262478.0\n",
       "2018-01-29  200.110000  200.860000  197.870000  198.230000    1876216.0\n",
       "2018-01-30  197.250000  197.730000  194.890000  196.900000    3034232.0\n",
       "2018-01-31  197.130000  200.960000  196.750000  199.760000    2514574.0\n",
       "2018-02-01  199.120000  201.750000  198.084500  199.380000    2353675.0\n",
       "\n",
       "[3292 rows x 5 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cut-off for where data transitions from training set to test set\n",
    "percent_train = 0.85\n",
    "data = adobeDf\n",
    "TRAIN_SPLIT = int(percent_train*len(data))\n",
    "print(TRAIN_SPLIT)\n",
    "\n",
    "full_data = len(adobeDf)\n",
    "print(full_data)\n",
    "\n",
    "features_considered = ['Adj. Open', 'Adj. High', 'Adj. Low', 'Adj. Close', 'Adj. Volume']\n",
    "\n",
    "features = adobeDf[features_considered]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.62426727, -0.63460294, -0.64110267, -0.66260519, -0.05483864],\n",
       "       [-0.64686887, -0.66642972, -0.70422182, -0.70923479,  0.39884239],\n",
       "       [-0.70723518, -0.70848653, -0.69355786, -0.71867514, -0.49395919],\n",
       "       ...,\n",
       "       [ 8.86194321,  8.8007382 ,  8.82109939,  8.84022113, -0.61432462],\n",
       "       [ 8.85507561,  8.984345  ,  8.92833564,  9.00388459, -0.73181638],\n",
       "       [ 8.9689633 ,  9.02925193,  9.00527476,  8.9821391 , -0.76819475]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scale features before training NN by subtractic mean and dividing by stdev of each feature\n",
    "#or use tf.keras.utils.normalize to rescale values to [0,1]\n",
    "dataset = features.values\n",
    "data_mean = dataset[:TRAIN_SPLIT].mean(axis = 0)\n",
    "data_std = dataset[:TRAIN_SPLIT].std(axis = 0)\n",
    "\n",
    "dataset = (dataset-data_mean)/data_std\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                     target_size, step, single_step = False):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    start_index = start_index+history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "        \n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "        \n",
    "        if single_step:\n",
    "            labels.append(dataset[indices])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "            \n",
    "    return np.array(data), np.array(labels)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#historic data -- not entirely sure how this works since we wanna do over all of train_split\n",
    "past_history = 1\n",
    "#how far into the future we want to predict a price\n",
    "future_target = 1\n",
    "#how we group the data -- could be 5 if we group by week\n",
    "#^^in which case future target woule be 1*5\n",
    "STEP = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_single, y_train_single = multivariate_data(dataset, dataset[:,1], 0, \n",
    "                                                   TRAIN_SPLIT, past_history, future_target, STEP,\n",
    "                                                  single_step = True)\n",
    "\n",
    "x_val_single, y_val_single = multivariate_data(dataset, dataset[:, 1],\n",
    "                                               TRAIN_SPLIT, None, past_history,\n",
    "                                               future_target, STEP,\n",
    "                                               single_step=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single window of past histroy = (1, 5)\n"
     ]
    }
   ],
   "source": [
    "print('single window of past histroy = {}'.format(x_train_single[0].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "BUFFER_SIZE = len(adobeDf)\n",
    "\n",
    "train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))\n",
    "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
    "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_step_model = tf.keras.models.Sequential()\n",
    "single_step_model.add(tf.keras.layers.LSTM(32, input_shape = x_train_single.shape[-2:]))\n",
    "single_step_model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "single_step_model.compile(optimizer = tf.keras.optimizers.RMSprop(), loss = 'mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "559/559 [==============================] - 5s 8ms/step - loss: 0.6722 - val_loss: 2.9988\n",
      "Epoch 2/10\n",
      "559/559 [==============================] - 3s 5ms/step - loss: 0.6671 - val_loss: 2.8810\n",
      "Epoch 3/10\n",
      "559/559 [==============================] - 3s 6ms/step - loss: 0.6670 - val_loss: 2.9506\n",
      "Epoch 4/10\n",
      "559/559 [==============================] - 3s 5ms/step - loss: 0.6649 - val_loss: 2.9862\n",
      "Epoch 5/10\n",
      "559/559 [==============================] - 3s 6ms/step - loss: 0.6674 - val_loss: 2.9301\n",
      "Epoch 6/10\n",
      "559/559 [==============================] - 4s 7ms/step - loss: 0.6666 - val_loss: 2.9887\n",
      "Epoch 7/10\n",
      "559/559 [==============================] - 4s 7ms/step - loss: 0.6686 - val_loss: 2.9949\n",
      "Epoch 8/10\n",
      "559/559 [==============================] - 4s 7ms/step - loss: 0.6656 - val_loss: 2.9611\n",
      "Epoch 9/10\n",
      "559/559 [==============================] - 3s 6ms/step - loss: 0.6669 - val_loss: 3.0141\n",
      "Epoch 10/10\n",
      "559/559 [==============================] - 2s 4ms/step - loss: 0.6654 - val_loss: 2.9885\n"
     ]
    }
   ],
   "source": [
    "#how often the epoch will run -- so like \n",
    "#we can do over a fraction of the data set instead of the whole thing each time\n",
    "#arbitraray-ish\n",
    "EVALUATION_INTERVAL = int(TRAIN_SPLIT/5)\n",
    "EPOCHS = 10\n",
    "\n",
    "single_step_history = single_step_model.fit(train_data_single, epochs=EPOCHS,\n",
    "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                            validation_data=val_data_single,\n",
    "                                            validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "##using the data table we created instead of one pulled from quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658\n"
     ]
    }
   ],
   "source": [
    "#pseudo-log-return\n",
    "def pseudoLogReturn(dataFrame, closingColumn, interval):\n",
    "    column = dataFrame[closingColumn]\n",
    "    rowNum = dataFrame.shape[0]\n",
    "    averages = []\n",
    "    for i in range(int(np.floor(rowNum/interval))):\n",
    "        data = column.iloc[i*interval:(i+1)*interval]\n",
    "        averages.append(np.mean(data))\n",
    "    if not np.isnan(np.mean(column[(i+1)*interval:])):\n",
    "        averages.append(np.mean(column[(i+1)*interval:]))\n",
    "    \n",
    "    result = []\n",
    "    for i in range(1,len(averages)):\n",
    "        temp = np.log(averages[i]/averages[i-1])*100\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "\n",
    "pseudo = pseudoLogReturn(adobeDf, \"Adj. Close\", 5)\n",
    "print(len(pseudo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658\n"
     ]
    }
   ],
   "source": [
    "#log return\n",
    "def logReturn(dataFrame, closingColumn, interval):\n",
    "    column = dataFrame[closingColumn]\n",
    "    rowNum = dataFrame.shape[0]\n",
    "    closingPrices = []\n",
    "    for i in range(interval-1,rowNum,interval):\n",
    "        closingPrices.append(column[i])\n",
    "    if i!=(rowNum-1):\n",
    "        closingPrices.append(column[-1])\n",
    "    \n",
    "    result = []\n",
    "    for i in range(1,len(closingPrices)):\n",
    "        temp = np.log(closingPrices[i]/closingPrices[i-1])*100\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "\n",
    "log = logReturn(adobeDf, \"Adj. Close\", 5)\n",
    "print(len(log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658\n"
     ]
    }
   ],
   "source": [
    "#5-day trend\n",
    "def trend(dataFrame, closingColumn, interval):\n",
    "    column = dataFrame[closingColumn]\n",
    "    rowNum = dataFrame.shape[0]\n",
    "    coeff = []\n",
    "    for i in range(int(np.floor(rowNum/interval))):\n",
    "        data = column.iloc[i*interval:(i+1)*interval]\n",
    "        x = np.linspace(1,interval,interval)\n",
    "        coeff.append(tuple(np.polyfit(x,data,1)))\n",
    "    #if not column[(i+1)*interval:].empty:\n",
    "        #x = np.linspace(1,rowNum-(i+1)*interval,rowNum-(i+1)*interval)\n",
    "        #data = column[(i+1)*interval:]\n",
    "        #coeff.append(tuple(np.polyfit(x,data,1)))\n",
    "    return coeff\n",
    "\n",
    "\n",
    "trendSlope = [coeff[0] for coeff in trend(adobeDf, \"Adj. Close\", 5)]\n",
    "print(len(trendSlope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658\n"
     ]
    }
   ],
   "source": [
    "#This creates lists of mean and standard deviation for each 5-day group\n",
    "\n",
    "def meanStd(dataFrame, price_column, interval, num_years):\n",
    "    weeks = list(range(0,52*num_years))\n",
    "    averages = []\n",
    "    stdevs = []\n",
    "    prices = []\n",
    "    for j in range(len(dataFrame)):\n",
    "        prices.append(dataFrame.iloc[j][price_column])\n",
    "    for i in range(1,len(prices)+1):\n",
    "        if i%interval == 0:\n",
    "            average = np.mean(prices[i-interval:i-1])\n",
    "            averages.append(average)\n",
    "            deviation = np.std(prices[i-interval:i-1])\n",
    "            stdevs.append(deviation)\n",
    "    #data = list(zip(weeks, averages, stdevs))\n",
    "    #df = pd.DataFrame(data, columns = ['Week', 'Avg Adj Close', 'St Dev'])\n",
    "    #return df\n",
    "    return (averages,stdevs)\n",
    "\n",
    "avg, std = meanStd(adobeDf, 'Adj. Close', 5, 13)\n",
    "avg, std\n",
    "print(len(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combines n arrays into single df\n",
    "def createDf(names, *argv):\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(names)):\n",
    "        df[names[i]] = argv[i]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pseudoLogReturn</th>\n",
       "      <th>logReturn</th>\n",
       "      <th>trend</th>\n",
       "      <th>average</th>\n",
       "      <th>standard deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.242345</td>\n",
       "      <td>-0.940098</td>\n",
       "      <td>-0.356933</td>\n",
       "      <td>30.021860</td>\n",
       "      <td>0.530707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.926988</td>\n",
       "      <td>-5.000161</td>\n",
       "      <td>-0.053990</td>\n",
       "      <td>29.262003</td>\n",
       "      <td>0.185488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.346076</td>\n",
       "      <td>2.689627</td>\n",
       "      <td>-0.339436</td>\n",
       "      <td>28.919567</td>\n",
       "      <td>0.242158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.488921</td>\n",
       "      <td>11.005948</td>\n",
       "      <td>0.105480</td>\n",
       "      <td>27.899759</td>\n",
       "      <td>0.143891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.600646</td>\n",
       "      <td>-0.078746</td>\n",
       "      <td>0.646878</td>\n",
       "      <td>30.944187</td>\n",
       "      <td>1.194310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>4.111478</td>\n",
       "      <td>5.441555</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>175.087500</td>\n",
       "      <td>0.422811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>4.070186</td>\n",
       "      <td>3.931632</td>\n",
       "      <td>1.898000</td>\n",
       "      <td>181.825000</td>\n",
       "      <td>2.825469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>3.729668</td>\n",
       "      <td>3.887893</td>\n",
       "      <td>1.946000</td>\n",
       "      <td>189.445000</td>\n",
       "      <td>3.342114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>0.542955</td>\n",
       "      <td>-1.607128</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>196.565000</td>\n",
       "      <td>0.784490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>0.616216</td>\n",
       "      <td>1.251657</td>\n",
       "      <td>-0.104000</td>\n",
       "      <td>198.705000</td>\n",
       "      <td>1.520140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>658 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pseudoLogReturn  logReturn     trend     average  standard deviation\n",
       "0          -2.242345  -0.940098 -0.356933   30.021860            0.530707\n",
       "1          -1.926988  -5.000161 -0.053990   29.262003            0.185488\n",
       "2          -2.346076   2.689627 -0.339436   28.919567            0.242158\n",
       "3          10.488921  11.005948  0.105480   27.899759            0.143891\n",
       "4           1.600646  -0.078746  0.646878   30.944187            1.194310\n",
       "..               ...        ...       ...         ...                 ...\n",
       "653         4.111478   5.441555  0.159000  175.087500            0.422811\n",
       "654         4.070186   3.931632  1.898000  181.825000            2.825469\n",
       "655         3.729668   3.887893  1.946000  189.445000            3.342114\n",
       "656         0.542955  -1.607128  0.884000  196.565000            0.784490\n",
       "657         0.616216   1.251657 -0.104000  198.705000            1.520140\n",
       "\n",
       "[658 rows x 5 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = createDf([\"pseudoLogReturn\",\"logReturn\",\"trend\",\"average\",\"standard deviation\"],\n",
    "         pseudo, log, trendSlope, avg, std)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559\n"
     ]
    }
   ],
   "source": [
    "data = features\n",
    "TRAIN_SPLIT = int(percent_train*len(data))\n",
    "print(TRAIN_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658\n"
     ]
    }
   ],
   "source": [
    "dataset = features.values\n",
    "data_mean = dataset[:TRAIN_SPLIT].mean(axis = 0)\n",
    "data_std = dataset[:TRAIN_SPLIT].std(axis = 0)\n",
    "\n",
    "dataset = (dataset-data_mean)/data_std\n",
    "dataset\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_single, y_train_single = multivariate_data(dataset, dataset[:,1], 0, \n",
    "                                                   TRAIN_SPLIT, past_history, future_target, STEP,\n",
    "                                                  single_step = True)\n",
    "\n",
    "x_val_single, y_val_single = multivariate_data(dataset, dataset[:, 1],\n",
    "                                               TRAIN_SPLIT, None, past_history,\n",
    "                                               future_target, STEP,\n",
    "                                               single_step=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single window of past histroy = (1, 5)\n"
     ]
    }
   ],
   "source": [
    "print('single window of past histroy = {}'.format(x_train_single[0].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))\n",
    "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
    "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_step_model = tf.keras.models.Sequential()\n",
    "single_step_model.add(tf.keras.layers.LSTM(32, input_shape = x_train_single.shape[-2:]))\n",
    "single_step_model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "single_step_model.compile(optimizer = tf.keras.optimizers.RMSprop(), loss = 'mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "Epoch 1/10\n",
      "111/111 [==============================] - 1s 11ms/step - loss: 0.6891 - val_loss: 1.5926\n",
      "Epoch 2/10\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 0.6844 - val_loss: 1.5764\n",
      "Epoch 3/10\n",
      "111/111 [==============================] - 1s 7ms/step - loss: 0.6862 - val_loss: 1.5964\n",
      "Epoch 4/10\n",
      "111/111 [==============================] - 1s 10ms/step - loss: 0.6877 - val_loss: 1.5608\n",
      "Epoch 5/10\n",
      "111/111 [==============================] - 1s 8ms/step - loss: 0.6796 - val_loss: 1.5909\n",
      "Epoch 6/10\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 0.6850 - val_loss: 1.5505\n",
      "Epoch 7/10\n",
      "111/111 [==============================] - 1s 8ms/step - loss: 0.6817 - val_loss: 1.5858\n",
      "Epoch 8/10\n",
      "111/111 [==============================] - 1s 7ms/step - loss: 0.6845 - val_loss: 1.5876\n",
      "Epoch 9/10\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 0.6835 - val_loss: 1.5740\n",
      "Epoch 10/10\n",
      "111/111 [==============================] - 1s 6ms/step - loss: 0.6813 - val_loss: 1.5620\n"
     ]
    }
   ],
   "source": [
    "EVALUATION_INTERVAL = int(TRAIN_SPLIT/5)\n",
    "print(EVALUATION_INTERVAL)\n",
    "\n",
    "single_step_history = single_step_model.fit(train_data_single, epochs=EPOCHS,\n",
    "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                            validation_data=val_data_single,\n",
    "                                            validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
