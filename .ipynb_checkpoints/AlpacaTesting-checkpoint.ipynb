{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modules needed for using alpaca and formatting our data\n",
    "import alpaca_trade_api as tradeapi\n",
    "import requests  \n",
    "import json\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tradeapi.REST(\n",
    "    key_id='PKFH5ZA4MRGSTSEFNQ3S',\n",
    "    secret_key='Yy5ygxRCE6bOwRRC03tQ9mYMudA6WQRKkRMbTnFY',\n",
    "    base_url='https://paper-api.alpaca.markets'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(symbol, timeframe, limit):\n",
    "    barset = api.get_barset(symbol, timeframe, limit)\n",
    "    return barset\n",
    "\n",
    "\n",
    "DATA = getData(\"ADBE\", \"day\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     open      high      low    close   volume\n",
      "0  449.84  450.7400  442.280  442.280  1561978\n",
      "1  448.26  449.0799  440.700  440.700  2346534\n",
      "2  449.09  454.0350  446.345  446.345  2065372\n",
      "3  451.35  464.3700  449.130  449.130  2066577\n",
      "4  460.87  461.7900  445.090  445.090  3049001\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#change data pulled into same format as data from quandl so that we may use our trained Model on it\n",
    "def changeFormat(data):\n",
    "    o = []\n",
    "    h = []\n",
    "    l = []\n",
    "    c = []\n",
    "    v = []\n",
    "    for d in data:\n",
    "        for i in data[d]:\n",
    "            o.append(i.o)\n",
    "            h.append(i.h)\n",
    "            l.append(i.l)\n",
    "            c.append(i.c)\n",
    "            v.append(i.v)\n",
    "    dataDict = {\"open\":o, \"high\":h, \"low\":l, \"close\":l, \"volume\":v}\n",
    "    df = pd.DataFrame(dataDict)\n",
    "    return df\n",
    "\n",
    "\n",
    "formatted_data = changeFormat(DATA)\n",
    "print(formatted_data)\n",
    "\n",
    "full_data = len(formatted_data)\n",
    "print(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.44317835, -0.86994955, -0.81567936, -0.81567936, -1.34808214],\n",
       "       [-0.78608813, -1.14435744, -1.34625712, -1.34625712,  0.26439341],\n",
       "       [-0.60595198, -0.32529922,  0.54938305,  0.54938305, -0.31347082],\n",
       "       [-0.11546076,  1.38303495,  1.48461031,  1.48461031, -0.31099422],\n",
       "       [ 1.95067922,  0.95657126,  0.12794312,  0.12794312,  1.70815377]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing Model 1 on Alpaca data\n",
    "\n",
    "#########this works, and should be all we need to test the saved Model 1\n",
    "dataset = formatted_data.values\n",
    "data_mean = dataset[:full_data].mean(axis = 0)\n",
    "data_std = dataset[:full_data].std(axis = 0)\n",
    "\n",
    "dataset = (dataset-data_mean)/data_std\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do multivariate data function on this to further organize for Model 1\n",
    "##had difficulty importing the below functions from Model_test.ipynb file\n",
    "\n",
    "######all this function shit may not be necessary, im not sure -- all pulled from Model_test file\n",
    "\n",
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                     target_size, step, single_step = False):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    start_index = start_index+history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "        \n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "        \n",
    "        if single_step:\n",
    "            labels.append(dataset[indices])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "            \n",
    "    return np.array(data), np.array(labels)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_history = 1\n",
    "future_target = 1\n",
    "STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_single, y_val_single = multivariate_data(dataset, dataset[:, 1],\n",
    "                                               0, full_data, past_history,\n",
    "                                               future_target, STEP,\n",
    "                                               single_step=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "BUFFER_SIZE = full_data\n",
    "\n",
    "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
    "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Found: <tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x643f2ef10>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8486c32857cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m############this works and imports our saved model into here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnew_model_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_model_1/original_df_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, loader_cls)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36m_finalize\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: <tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x643f2ef10>"
     ]
    }
   ],
   "source": [
    "############this works and imports our saved model into here\n",
    "\n",
    "new_model_1 = tf.keras.models.load_model('saved_model_1/original_df_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_model_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1e3e5fc8be02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m####using processed data 'val_data_single' instead of normalized, raw 'dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_model_1' is not defined"
     ]
    }
   ],
   "source": [
    "####using processed data 'val_data_single' instead of normalized, raw 'dataset'\n",
    "predictions = new_model_1.predict(val_data_single, steps=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = (dataset-data_mean)/data_std\n",
    "\n",
    "def price(predictions):\n",
    "    prices = []\n",
    "    for prediction in predictions:\n",
    "        price = prediction*data_std+data_mean\n",
    "        prices.append(price)\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([4.37240451e+02, 4.45937709e+02, 4.34664896e+02, 4.34664896e+02,\n",
       "        3.96206590e+07]),\n",
       " array([4.37163479e+02, 4.45866560e+02, 4.34625355e+02, 4.34625355e+02,\n",
       "        3.95045048e+07]),\n",
       " array([4.37265562e+02, 4.45960920e+02, 4.34677795e+02, 4.34677795e+02,\n",
       "        3.96585524e+07]),\n",
       " array([4.39315535e+02, 4.47855819e+02, 4.35730878e+02, 4.35730878e+02,\n",
       "        4.27520600e+07])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
